---
title: Week - 6 Reading and Videos
tags: test
Creation Time: April 11, 2022 11:11 AM
Last Update: April 28, 2022 5:00 AM
Notes: https://amritauniv.sharepoint.com/sites/CyberML/SitePages/Week-6.aspx
Source: Recorded Videos, Sharepoint
Week: Week 6
---





<!--more-->

## Decision Tree - 1 :

- Decision tree are collection of if-else statements
- We’ll split the given dataset unitl we reach the finest form of what we’re looking for
- certain features will be better for making a decision than others.

## Decision Tree - 2 :

- we want the splits to give us more refined data after every split, so we need to choose our attribute(s) carefully.
- This split is recursive DUH!
- We’ll use Greedy Algo techniques to split the dataset and get a decision tree.
- The Goal is to get a best possible split - Greedy Algo!

### Pseudocode :

```
Step 1 : Check for Base Case
Step 2 : For each feature **A**, calculate "*splitting value*" (it's a measure of how good would it be to split on this value)
Step 3 : Let **A_best** be the feature w highest *"Spllitting Value"*
Step 4 : Create a decision node that splits up on **A_best**.
Step 5 : Repeat Step 1 through 5 for Left Subtree.
Step 6 : Repeat Step 1 through 6 for Right Subtree.
```

### Base Cases :

This is the case when splitting stops, i.e. we reach the leaf node or we stop splitting

It happens when  :

1. All the sample (~ 90%) belong to the same class. 

OR

1. None of the features provide any “value”. i.e. we can’t find a good split - this is where we need heuristics. [clever human idea] - Basically we need to set a limit to the purity/refiness we need for our dataset. 

> **NOTE :  toooo many splits will lead to overfitting of the data , so learn to settle down.**
> 

### Properties of Decision Trees

### Pros

- Training and inference is fast AF
- required mem space is samll
- can handel categorical data w/o modifications
- can handel multi class class w/o modifications
- scaling data is ***not*** required
- White box model - ***Interpretability***
- We can “LOOK and SEE” why a decision was made.

### Cons

- Tends towards very high complexity - **OVerfitting**

## Splitting Mechanisms :

A Good split maximizes ***PURITY!***

### Gini  Impurity Coefficient :

“*a measure of how often a randomly choosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset”  - **WTF**!*

$== 1 - \sum\limits_{i=1}^{j}p_{i}^2$             This  $\epsilon\ [0, 0.5]$ 

Where, 

$j == Classes$

$p_i == probability\ of\ labelling\ right$

So we need to calculate Gini Impurity for both the subtress we get after each split to know if we were really able to purify the data or not 

### Information Gain Ratio :

- The key here is ***ENTROPY***
- we want to reduce the uncertainity, therefore we want to reduce entropy :
    
    $Entropy\ of\ parent - sum(entropy\ of\ children)$
    

*“Information gain ratio is what information we gained by doing the split”*

### What is it all about ?

***PURITY***

We need to refine the data so as to get as pure as possible data which we wanted on every split on each new subnode.

**Decision tree will iterate over all the features looking for the split that maximizes purity.**


---

If you like TeXt, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/kitian616/jekyll-TeXt-theme.svg?label=Stars&style=social)](https://github.com/kitian616/jekyll-TeXt-theme/)
